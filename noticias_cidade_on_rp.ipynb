{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY7oqBQzpmOh",
        "outputId": "02245760-db65-4add-9be3-915f9e57991a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraindo dados da página 1\n",
            "Extraindo dados da página 2\n",
            "Extraindo dados da página 3\n",
            "Extraindo dados da página 4\n",
            "Extraindo dados da página 5\n",
            "Extraindo dados da página 6\n",
            "Extraindo dados da página 7\n",
            "Extraindo dados da página 8\n",
            "Extraindo dados da página 9\n",
            "Extraindo dados da página 10\n",
            "Extraindo dados da página 11\n",
            "Extraindo dados da página 12\n",
            "Extraindo dados da página 13\n",
            "Extraindo dados da página 14\n",
            "Extraindo dados da página 15\n",
            "Extraindo dados da página 16\n",
            "Extraindo dados da página 17\n",
            "Extraindo dados da página 18\n",
            "Extraindo dados da página 19\n",
            "Extraindo dados da página 20\n",
            "Extraindo dados da página 21\n",
            "Extraindo dados da página 22\n",
            "Extraindo dados da página 23\n",
            "Extraindo dados da página 24\n",
            "Extraindo dados da página 25\n",
            "Extraindo dados da página 26\n",
            "Extraindo dados da página 27\n",
            "Extraindo dados da página 28\n",
            "Extraindo dados da página 29\n",
            "Extraindo dados da página 30\n",
            "Extraindo dados da página 31\n",
            "Extraindo dados da página 32\n",
            "Extraindo dados da página 33\n",
            "Extraindo dados da página 34\n",
            "Extraindo dados da página 35\n",
            "Extraindo dados da página 36\n",
            "Extraindo dados da página 37\n",
            "Extraindo dados da página 38\n",
            "Extraindo dados da página 39\n",
            "Extraindo dados da página 40\n",
            "Extraindo dados da página 41\n",
            "Extraindo dados da página 42\n",
            "Extraindo dados da página 43\n",
            "Extraindo dados da página 44\n",
            "Extraindo dados da página 45\n",
            "Extraindo dados da página 46\n",
            "Extraindo dados da página 47\n",
            "Extraindo dados da página 48\n",
            "Extraindo dados da página 49\n",
            "Extraindo dados da página 50\n",
            "Dados extraídos e salvos em 'data/noticias.json'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# função responsável por extrair os dados de uma página de notícias\n",
        "def extrair_dados_noticia(url):\n",
        "    # fazendo a requisição http para obter o conteúdo da página\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')  # parseando o html com beautifulsoup\n",
        "\n",
        "    noticias = []  # lista que armazenará as notícias extraídas\n",
        "\n",
        "    # itera sobre cada bloco de notícia encontrado na página\n",
        "    for noticia in soup.find_all('div', class_='tdb_module_loop'):\n",
        "        try:\n",
        "            # extraindo o link da notícia\n",
        "            link = noticia.find('a', href=True)['href']\n",
        "\n",
        "            # extraindo o título da notícia\n",
        "            titulo = noticia.find('a', title=True)['title']\n",
        "\n",
        "            # extraindo e formatando a data no padrão iso 8601\n",
        "            data_elemento = noticia.find('time', class_='entry-date')['datetime']\n",
        "            data = datetime.strptime(data_elemento, \"%Y-%m-%dT%H:%M:%S%z\").isoformat()\n",
        "\n",
        "            # extraindo a categoria e criando uma descrição detalhada\n",
        "            categoria = noticia.find('a', class_='td-post-category').text.strip()\n",
        "            descricao = f\"{categoria} - {titulo}\"\n",
        "\n",
        "            # coletando tags (usando a categoria como referência)\n",
        "            tags = [categoria]\n",
        "\n",
        "            # estrutura de dados da notícia\n",
        "            noticia_dados = {\n",
        "                \"url\": link,\n",
        "                \"titulo\": titulo,\n",
        "                \"descricao\": descricao,\n",
        "                \"tags\": tags,\n",
        "                \"texto\": \"Texto completo pode ser capturado com request adicional se necessario\",\n",
        "                \"data\": data\n",
        "            }\n",
        "\n",
        "            # adicionando a notícia à lista\n",
        "            noticias.append(noticia_dados)\n",
        "\n",
        "        # tratamento de erro caso algum dado não seja encontrado\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar uma noticia: {e}\")\n",
        "\n",
        "    return noticias  # retorna a lista de notícias extraídas\n",
        "\n",
        "\n",
        "# função principal do script\n",
        "def main():\n",
        "    # url base que será formatada com o número da página\n",
        "    base_url = \"https://www.acidadeon.com/ribeiraopreto/economia/pagina/{}/\"\n",
        "\n",
        "    dados_extraidos = []  # lista para armazenar todas as notícias extraídas\n",
        "\n",
        "    # loop para percorrer as páginas (da 1 até a 50)\n",
        "    for pagina in range(1, 51):\n",
        "        print(f\"Extraindo dados da página {pagina}\")  # log de progresso\n",
        "        url = base_url.format(pagina)  # formata a url com o número da página\n",
        "        dados_pagina = extrair_dados_noticia(url)  # extrai os dados da página atual\n",
        "        dados_extraidos.extend(dados_pagina)  # adiciona os dados à lista principal\n",
        "\n",
        "    # verifica se o diretório 'data' existe, caso contrário, cria-o\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "\n",
        "    # salva os dados extraídos em um arquivo json dentro do diretório 'data'\n",
        "    with open(\"data/noticias.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(dados_extraidos, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"Dados extraídos e salvos em 'data/noticias.json'\")  # confirmação ao usuário\n",
        "\n",
        "\n",
        "# ponto de entrada do script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}